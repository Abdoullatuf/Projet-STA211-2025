{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kE1OkvRslq0w"
      },
      "outputs": [],
      "source": [
        "# Notebook: 03_Intermediate_Preprocessing.ipynb\n",
        "# Objectif : proposer plusieurs pré-traitements alternatifs vus en STA211,\n",
        "# puis exporter des jeux de données prêts pour modélisation afin de comparer les F1-scores.\n",
        "\n",
        "# %%\n",
        "# 1. Import des bibliothèques\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "import prince\n",
        "from minisom import MiniSom  # si package installé\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.feature_extraction import FeatureHasher\n",
        "\n",
        "# %%\n",
        "# 2. Chargement des données transformées\n",
        "df = pd.read_csv('data_transformed.csv')\n",
        "\n",
        "# %%\n",
        "# 3. Préparer la cible et les colonnes\n",
        "y = df['outcome'].map({'ad.':1, 'noad.':0})\n",
        "features_num = ['X1','X2','X3','X4']\n",
        "features_bin = [c for c in df.columns if c not in features_num+['outcome']]\n",
        "\n",
        "# %%\n",
        "# 4. Scénarios de pré-traitement intermédiaires\n",
        "os.makedirs('processed', exist_ok=True)\n",
        "\n",
        "# 4.1 Discrétisation MDLPC (via arbre binaire - approximée par KBins + entropie)\n",
        "disc = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n",
        "df_disc = df.copy()\n",
        "df_disc[features_num] = disc.fit_transform(df[features_num])\n",
        "df_disc.to_csv('processed/data_disc.csv', index=False)\n",
        "\n",
        "# 4.2 Classification hiérarchique de variables (CAH) pour regrouper continues\n",
        "corr = df[features_num].corr().abs()\n",
        "cluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n",
        "labels = cluster.fit_predict(1-corr)\n",
        "# Créer moyennes par groupe\n",
        "df_cah = df.copy()\n",
        "for grp in np.unique(labels):\n",
        "    cols = [features_num[i] for i in range(len(features_num)) if labels[i]==grp]\n",
        "    df_cah[f'group_{grp}'] = df[cols].mean(axis=1)\n",
        "# drop originales ou non?\n",
        "# ici on conserve nouveaux et binaires\n",
        "cols_keep = [f'group_0','group_1'] + features_bin + ['outcome']\n",
        "df_cah[cols_keep].to_csv('processed/data_cah.csv', index=False)\n",
        "\n",
        "# 4.3 AFM (pour variables mixtes)\n",
        "am = prince.MFA(\n",
        "    groups=[len(features_num), len(features_bin)],\n",
        "    group_names=['quant','bin'],\n",
        "    n_components=2,\n",
        "    random_state=42\n",
        ")\n",
        "am = am.fit(df[features_num + features_bin])\n",
        "coords_mfa = am.transform(df[features_num + features_bin])\n",
        "df_mfa = pd.concat([pd.DataFrame(coords_mfa, columns=['MFA1','MFA2']), df[features_bin].reset_index(drop=True)], axis=1)\n",
        "df_mfa.to_csv('processed/data_mfa.csv', index=False)\n",
        "\n",
        "# 4.4 CARTES DE KOHONEN (SOM)\n",
        "# Normaliser X1-X4\n",
        "df_som = df.copy()\n",
        "X = (df_som[features_num] - df_som[features_num].mean())/df_som[features_num].std()\n",
        "som = MiniSom(x=3, y=3, input_len=4, sigma=1.0, learning_rate=0.5)\n",
        "som.random_weights_init(X.values)\n",
        "som.train_random(X.values, 100)\n",
        "bmus = np.array([som.winner(x) for x in X.values])\n",
        "df_som['SOM_cluster'] = bmus[:,0]*3 + bmus[:,1]\n",
        "df_som.to_csv('processed/data_som.csv', index=False)\n",
        "\n",
        "# 4.5 Imputation multiple MICE\n",
        "imp = IterativeImputer(random_state=42)\n",
        "df_mice = df.copy()\n",
        "df_mice[features_num] = imp.fit_transform(df_mice[features_num])\n",
        "df_mice.to_csv('processed/data_mice.csv', index=False)\n",
        "\n",
        "# %%\n",
        "# 5. Résumé des fichiers générés\n",
        "print(\"Fichiers générés :\")\n",
        "for f in os.listdir('processed'):\n",
        "    print(\"- processed/\" + f)\n",
        "\n",
        "# Fin du notebook 03_Intermediate_Preprocessing.ipynb\n"
      ]
    }
  ]
}